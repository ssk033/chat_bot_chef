# ðŸŽ‰ Fully Local AI Recipe Chatbot

This project now runs **100% locally** - no external APIs, no costs, complete privacy!

## What Changed

âœ… **Embeddings**: Custom trained model (no Google API)  
âœ… **Chat/LLM**: Ollama local LLM (no Gemini API)  
âœ… **TTS**: Browser Web Speech API (no external services)  

## Quick Start

### 1. Install Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Or download from https://ollama.ai
```

### 2. Pull a Model

```bash
# Lightweight (recommended for testing)
ollama pull llama3.2:1b

# Or better quality
ollama pull llama3.2:3b
```

### 3. Start Ollama

```bash
ollama serve
```

### 4. Train Your Embedding Model (One-time)

```bash
# Install Python dependencies
pip install -r requirements.txt

# Train the model
npm run train
```

### 5. Load Recipes

```bash
npm run load
```

### 6. Start the App

```bash
npm run dev
```

## Configuration

Optional: Set model in `.env`:

```env
OLLAMA_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434
```

## Features

- ðŸ”’ **100% Private** - All data stays on your machine
- ðŸ’° **Free** - No API costs
- âš¡ **Fast** - Local processing
- ðŸŽ¯ **Custom** - Trained on your recipe data
- ðŸ”Š **TTS** - Browser-based text-to-speech

## Troubleshooting

See `OLLAMA_SETUP.md` for detailed troubleshooting.

## Architecture

```
User Query
    â†“
Custom Embedding Model (Python) â†’ Vector Search â†’ Find Recipes
    â†“
Ollama LLM (Local) â†’ Generate Response
    â†“
Browser TTS â†’ Speak Response
```

All processing happens locally! ðŸš€

